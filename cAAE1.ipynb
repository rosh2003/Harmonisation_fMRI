{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class cAAE(nn.Module):\n",
    "    def __init__(self, input_dim=13456, latent_dim=128):\n",
    "        super(cAAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Discriminator (adversarial network)\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Classifier branch\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def discriminate(self, z):\n",
    "        return self.discriminator(z)\n",
    "    \n",
    "    def classify(self, z):\n",
    "        return self.classifier(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        reconstructed = self.decode(z)\n",
    "        discriminated = self.discriminate(z)\n",
    "        classified = self.classify(z)\n",
    "        return reconstructed, discriminated, classified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the cAAE\n",
    "def train_caae(model, data_loader, num_epochs=50, learning_rate=0.001, classification_weight=5.0, adversarial_weight=1.0,patience=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    discriminator_loss_fn = nn.BCELoss()\n",
    "    reconstruction_loss_fn = nn.MSELoss()\n",
    "    classification_loss_fn = nn.BCELoss()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            z = model.encode(inputs)\n",
    "            reconstructed, discriminated, classified = model(inputs)\n",
    "            \n",
    "            # Discriminator targets: 1 for real (true site), 0 for fake (predicted site)\n",
    "            real_labels = torch.ones_like(discriminated)\n",
    "            fake_labels = torch.zeros_like(discriminated)\n",
    "\n",
    "            # Losses\n",
    "            reconstruction_loss = reconstruction_loss_fn(reconstructed, inputs)\n",
    "            classification_loss = classification_loss_fn(classified, labels)\n",
    "            adversarial_loss = discriminator_loss_fn(discriminated, fake_labels)\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss = (\n",
    "                reconstruction_loss\n",
    "                + classification_weight * classification_loss\n",
    "                + adversarial_weight * adversarial_loss\n",
    "            )\n",
    "\n",
    "            # Backpropagation\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        # Early stopping logic\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('Sheet 1-ABIDE_metadata.csv')\n",
    "abide_df = meta_data[['Subject', 'Site', 'Sex', 'Age']]\n",
    "\n",
    "def extract_subject_id(file_path):\n",
    "    # Assumes the subject ID is always after 'sub-control' or 'sub-patient' and is a numeric value\n",
    "    base_name = os.path.basename(file_path)  # Get the file name (e.g., 'sub-control50197_AAL116_correlation_matrix.mat')\n",
    "    subject_id = base_name.split('_')[0].replace('sub-control', '').replace('sub-patient', '')  # Extract the subject ID\n",
    "    return int(subject_id)\n",
    "control_path = '/Users/roshan/Desktop/fMRI/abide/'\n",
    "# Use glob to search for the specific .mat file recursively\n",
    "specific_files = glob.glob(os.path.join(control_path, '**', '*_AAL116_correlation_matrix.mat'), recursive=True)\n",
    "\n",
    "patient_files = [f for f in specific_files if 'patient' in os.path.dirname(f)]\n",
    "control_files = [f for f in specific_files if 'control' in os.path.dirname(f)]\n",
    "\n",
    "all_files = control_files+patient_files\n",
    "\n",
    "def load_matrix(file_path):\n",
    "    mat_data = loadmat(file_path)\n",
    "    return mat_data['data'] \n",
    "\n",
    "data_matrices = []\n",
    "labels = []\n",
    "subject_ids = []\n",
    "site_info = []\n",
    "\n",
    "# Process control files (label = 0)\n",
    "for file_path in control_files:\n",
    "    subject_id = extract_subject_id(file_path)\n",
    "    \n",
    "    # Look up site information from `abide_df` using subject ID\n",
    "    site = abide_df.loc[abide_df['Subject'] == subject_id, 'Site'].values[0]\n",
    "    \n",
    "    # Load the matrix and flatten it\n",
    "    matrix = load_matrix(file_path)\n",
    "    data_matrices.append(matrix.flatten())\n",
    "    labels.append(0)  # Control label\n",
    "    subject_ids.append(subject_id)\n",
    "    site_info.append(site)\n",
    "\n",
    "# Process autism files (label = 1)\n",
    "for file_path in patient_files:\n",
    "    subject_id = extract_subject_id(file_path)\n",
    "    \n",
    "    # Look up site information from `abide_df` using subject ID\n",
    "    site = abide_df.loc[abide_df['Subject'] == subject_id, 'Site'].values[0]\n",
    "    \n",
    "    # Load the matrix and flatten it\n",
    "    matrix = load_matrix(file_path)\n",
    "    data_matrices.append(matrix.flatten())\n",
    "    labels.append(1)  # Autism label\n",
    "    subject_ids.append(subject_id)\n",
    "    site_info.append(site)\n",
    "\n",
    "data_matrices = np.array(data_matrices)\n",
    "labels = np.array(labels)\n",
    "site_info = np.array(site_info)\n",
    "\n",
    "# Prepare the site DataFrame for ComBat harmonization\n",
    "site_df = pd.DataFrame({'site': site_info})\n",
    "copy_labels = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.6538\n",
      "Epoch [2/50], Loss: 3.0521\n",
      "Epoch [3/50], Loss: 2.3924\n",
      "Epoch [4/50], Loss: 1.6765\n",
      "Epoch [5/50], Loss: 1.2382\n",
      "Epoch [6/50], Loss: 0.5102\n",
      "Epoch [7/50], Loss: 0.2200\n",
      "Epoch [8/50], Loss: 0.4146\n",
      "Epoch [9/50], Loss: 0.4333\n",
      "Epoch [10/50], Loss: 0.1059\n",
      "Epoch [11/50], Loss: 0.1767\n",
      "Epoch [12/50], Loss: 0.4121\n",
      "Epoch [13/50], Loss: 0.4424\n",
      "Epoch [14/50], Loss: 0.1121\n",
      "Epoch [15/50], Loss: 0.1961\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "    data_matrices, labels, range(len(labels)), test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Ensure labels are column vectors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)    # Ensure labels are column vectors\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the test set \n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Instantiate the model \n",
    "input_dim = X_train_tensor.shape[1]  # This should be 13456 (116x116 flattened)\n",
    "latent_dim = 128  # can be changed\n",
    "\n",
    "model = cAAE(input_dim=input_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_caae(\n",
    "    model,\n",
    "    data_loader=train_loader, \n",
    "    num_epochs=50, \n",
    "    learning_rate=0.001, \n",
    "    classification_weight=5.0,  # Adjust based on previous results\n",
    "    adversarial_weight=1.0      # experimented with changing this\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonized data info saved to: /Users/roshan/Desktop/fMRI/output/caae1/harmonized_data_info.csv\n"
     ]
    }
   ],
   "source": [
    "# Harmonize the test data and save the results \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    harmonized_test_data, _, _ = model(X_test_tensor)\n",
    "    harmonized_test_data = harmonized_test_data.numpy()\n",
    "\n",
    "# Reshape and save the harmonized matrices \n",
    "harmonized_matrices = harmonized_test_data.reshape(-1, 116, 116)\n",
    "\n",
    "# Save the harmonized matrices \n",
    "output_dir = '/Users/roshan/Desktop/fMRI/output/caae1/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_paths = []\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    subject_id = subject_ids[idx]\n",
    "    label = copy_labels[idx]\n",
    "    site = site_info[idx]\n",
    "\n",
    "    prefix = 'sub-control' if label == 0 else 'sub-patient'\n",
    "    file_name = f'{prefix}{subject_id}_harmonized.mat'\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    savemat(file_path, {'data': harmonized_matrices[i]})\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "output_csv = '/Users/roshan/Desktop/fMRI/output/caae1/harmonized_data_info.csv'\n",
    "csv_df = pd.DataFrame({\n",
    "    'file_path': file_paths,\n",
    "    'subject_id': [subject_ids[idx] for idx in test_indices],\n",
    "    'autism_label': [copy_labels[idx] for idx in test_indices],\n",
    "    'site': [site_info[idx] for idx in test_indices]\n",
    "})\n",
    "\n",
    "csv_df.to_csv(output_csv, index=False)\n",
    "print(f\"Harmonized data info saved to: {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
